{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See instructions based on <em> ../coverages.ipynb </em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries.metrics import TimeSeriesScorer\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from plotnine import *\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from typing import Any, Dict, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 1. Helper function: compute coverage for one prediction window.\n",
    "########################################################################\n",
    "\n",
    "def help_calc_coverages_for_1_window(\n",
    "    target: str,\n",
    "    test_data: TimeSeriesDataFrame,\n",
    "    prediction_length: int,\n",
    "    predictions: TimeSeriesDataFrame\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - test_data: a TimeSeriesDataFrame whose target column holds the observerd values.\n",
    "      - predictions: a TimeSeriesDataFrame containing forecasts for the last prediction_length\n",
    "                     time points in test_data. Its next-to-last column holds the lower\n",
    "                     prediction bound and its last column holds the upper bound.\n",
    "    \n",
    "    This function computes the coverage of the prediction intervals (i.e. the fraction of\n",
    "    true values that lie between the lower and upper bounds).\n",
    "    \"\"\"\n",
    "    # Get the true values for the test period.\n",
    "    # (Assume that test_data is ordered so that its last prediction_length rows are the test set.)\n",
    "    obs_values = test_data[target].iloc[-prediction_length:]\n",
    "    \n",
    "    # The next-to-last column contains the lower bound and the last column the upper bound.\n",
    "    lower_bounds = predictions.iloc[:, -2]\n",
    "    upper_bounds = predictions.iloc[:, -1]\n",
    "\n",
    "    print(pd.DataFrame({\"Obs\":obs_values, predictions.columns[1]:lower_bounds, predictions.columns[2]:upper_bounds})) # for checking\n",
    "\n",
    "    # Compute the fraction of true values that fall within the interval.\n",
    "    in_interval = ((obs_values >= lower_bounds) & (obs_values <= upper_bounds)).mean()\n",
    "\n",
    "    return float(in_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "# 2. Compute coverages on one window using one (or more) models.\n",
    "########################################################################\n",
    "\n",
    "def calc_coverages_for_1_window(\n",
    "    window_data: pd.DataFrame,\n",
    "    timestamp_col: str,\n",
    "    target: str | None,\n",
    "    prediction_length: int,\n",
    "    eval_metric: str | TimeSeriesScorer | None,\n",
    "    ci_level: float,\n",
    "    time_limit: int | None,\n",
    "    hyperparameters: Dict[str | Type, Any]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a block of data (window_data) with observation times (in timestamp_col)\n",
    "    and targets (in target), this function splits the window into training and test parts.\n",
    "    The training part is the first (window length - prediction_length) observations and\n",
    "    the test part is all the window length observations.\n",
    "    \n",
    "    For each model specified in the hyperparameters dict, a TimeSeriesPredictor is trained\n",
    "    (with the given eval_metric and within time_limit seconds) and used to produce forecasts\n",
    "    along with ci_level prediction intervals. The test-set coverage is computed\n",
    "    using help_calc_coverages_for_1_window.\n",
    "    \n",
    "    The function returns a DataFrame with one row per model and columns:\n",
    "       test_start_time, test_end_time, model, coverage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assume window_data is already modified with timestamp, target columns.\n",
    "    # Add item_id col\n",
    "    window_data[\"item_id\"] = 0\n",
    "    # Create AutoGluon TimeSeriesDataFrame objects.\n",
    "    window_ts = TimeSeriesDataFrame.from_data_frame(window_data)\n",
    "\n",
    "    # Split the window into training and test data.\n",
    "    train_df = window_data.iloc[:-prediction_length].copy()\n",
    "    test_df = window_data.iloc[-prediction_length:].copy() # ease for getting start & end time\n",
    "\n",
    "    train_ts, test_ts = window_ts.train_test_split(prediction_length) # predictor & helper function\n",
    "\n",
    "    # Get the start and end times of the test period.\n",
    "    test_start_time = test_df[timestamp_col].iloc[0]\n",
    "    test_end_time   = test_df[timestamp_col].iloc[-1]\n",
    "    \n",
    "    # Compute the quantile levels corresponding to the desired ci_level.\n",
    "    # For example, if ci_level=0.95, we use lower quantile 0.025 and upper quantile 0.975.\n",
    "    lower_quantile = (1 - ci_level) / 2\n",
    "    upper_quantile = 1 - lower_quantile\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over models specified in hyperparameters.\n",
    "    for model_name, params in hyperparameters.items():\n",
    "\n",
    "        # Infer the frequency from the training timestamps.\n",
    "        freq = pd.infer_freq(train_df[timestamp_col])\n",
    "        \n",
    "        # Create a predictor.\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length = prediction_length,\n",
    "            target = target,\n",
    "            eval_metric = eval_metric,\n",
    "            freq = freq,\n",
    "            quantile_levels = [lower_quantile, upper_quantile]\n",
    "        )\n",
    "        \n",
    "        # Fit the predictor using only this model's hyperparameters.\n",
    "        predictor.fit(train_ts, hyperparameters = {model_name: params}, time_limit = time_limit)\n",
    "        \n",
    "        # Get predictions including quantile forecasts.\n",
    "        predictions = predictor.predict(train_ts)\n",
    "        \n",
    "        # Compute the coverage using our helper function.\n",
    "        coverage = help_calc_coverages_for_1_window(target, test_ts, prediction_length, predictions)\n",
    "        \n",
    "        results.append({\n",
    "            \"test_start_time\": test_start_time,\n",
    "            \"test_end_time\": test_end_time,\n",
    "            \"model\": model_name,\n",
    "            \"coverage\": coverage\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results, \n",
    "                        columns = [\"test_start_time\", \"test_end_time\", \"model\", \"coverage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 3. Compute coverages across many windows.\n",
    "########################################################################\n",
    "\n",
    "def calc_coverages(\n",
    "    data: pd.DataFrame,\n",
    "    train_size: int,\n",
    "    prediction_length: int,\n",
    "    stride: int,\n",
    "    timestamp_col: str,\n",
    "    target: str | None,\n",
    "    eval_metric: str | TimeSeriesScorer | None,\n",
    "    ci_level: float,\n",
    "    time_limit: int | None,\n",
    "    hyperparameters: Dict[str | Type, Any]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carves data into overlapping windows. Each window has train_size + prediction_length\n",
    "    observations. The first train_size observations are used for training and the final\n",
    "    prediction_length for testing. Windows are separated by stride.\n",
    "    \n",
    "    For each window and for each model in hyperparameters, the test-set coverage is computed\n",
    "    (using calc_coverages_for_1_window). The results are returned in a DataFrame with columns:\n",
    "       test_start_time, test_end_time, model, coverage.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_window_length = train_size + prediction_length\n",
    "    \n",
    "    # Loop over all starting indices for windows that fit in data.\n",
    "    for start in range(0, len(data) - total_window_length + 1, stride):\n",
    "\n",
    "        window_data = data.iloc[start : start + total_window_length].copy()\n",
    "\n",
    "        window_coverages = calc_coverages_for_1_window(\n",
    "            window_data = window_data,\n",
    "            timestamp_col = timestamp_col,\n",
    "            target = target,\n",
    "            prediction_length = prediction_length,\n",
    "            eval_metric = eval_metric,\n",
    "            ci_level = ci_level,\n",
    "            time_limit = time_limit,\n",
    "            hyperparameters = hyperparameters\n",
    "        )\n",
    "\n",
    "        results.append(window_coverages)\n",
    "    \n",
    "    if results:\n",
    "        return pd.concat(results, ignore_index=True)[[\"test_start_time\", \"test_end_time\", \"model\", \"coverage\"]]\n",
    "    else:\n",
    "        # If no window fits, return an empty DataFrame with the correct columns.\n",
    "        return pd.DataFrame(columns=[\"test_start_time\", \"test_end_time\", \"model\", \"coverage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 4. Simulate an AR(1) process.\n",
    "########################################################################\n",
    "\n",
    "def simulate_ar1(phi: float, sigma: float, n: int, rng: np.random.Generator) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate a Gaussian AR(1) process defined by:\n",
    "         y[t] = phi * y[t-1] + epsilon[t],\n",
    "    where the epsilon's are independent N(0,sigma^2) random variables.\n",
    "    \n",
    "    The process is stationary only if |phi| < 1.\n",
    "    A ValueError is raised if |phi| >= 1.\n",
    "    \n",
    "    Returns a DataFrame with two columns:\n",
    "      - timestamp: starting at \"2020-01-01 00:00:00\" with a one-minute frequency.\n",
    "      - target: the simulated time series.\n",
    "    \"\"\"\n",
    "    if abs(phi) >= 1:\n",
    "        raise ValueError(\"The AR(1) process is stationary only if |phi| < 1.\")\n",
    "    \n",
    "    # The ARMA process in statsmodels is specified with the convention:\n",
    "    #    ar_params are the coefficients for L^1, L^2, ... in the polynomial:\n",
    "    #         1 - phi * L\n",
    "    # so we set ar = [1, -phi] and ma = [1].\n",
    "    ar = np.array([1, -phi])\n",
    "    ma = np.array([1])\n",
    "    arma_process = ArmaProcess(ar, ma)\n",
    "    \n",
    "    # Generate a sample of length n.\n",
    "    sample = arma_process.generate_sample(nsample=n, scale=sigma, distrvs = lambda size: rng.standard_normal(size))\n",
    "    \n",
    "    # Create a timestamp series (one-minute steps).\n",
    "    timestamps = pd.date_range(start=\"2020-01-01 00:00:00\", periods=n, freq='T')\n",
    "    \n",
    "    return pd.DataFrame({\"timestamp\": timestamps, \"target\": sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 5. Compute phi from FVE.\n",
    "########################################################################\n",
    "\n",
    "def calc_phi_from_fve(fve: float) -> float:\n",
    "    \"\"\"\n",
    "    For the AR(1) model y[t] = phi*y[t-1] + epsilon[t] (with stationary variance),\n",
    "    one can show that the optimal one‐step linear predictor is phi*y[t-1] and that\n",
    "    the fraction of variance explained (FVE) is given by\n",
    "    \n",
    "         FVE = phi^2.\n",
    "    \n",
    "    This function returns the nonnegative phi (i.e. sqrt(fve)) that yields the given FVE.\n",
    "    A ValueError is raised if fve is not in the interval [0, 1).\n",
    "    \"\"\"\n",
    "    if not (0 <= fve < 1):\n",
    "        raise ValueError(\"fve must be in the interval [0, 1).\")\n",
    "    return np.sqrt(fve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24008\\1695284246.py:33: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "Beginning AutoGluon training... Time limit = 60s\n",
      "AutoGluon will save models to 'c:\\Users\\Administrator\\Desktop\\URPS\\urps_2025\\notebooks\\Jingyi_notebooks\\AutogluonModels\\ag-20250204_184713'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.95 GB / 15.73 GB (31.5%)\n",
      "Disk Space Avail:   72.76 GB / 200.00 GB (36.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'freq': 'min',\n",
      " 'hyperparameters': {'AutoARIMA': {}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 100,\n",
      " 'quantile_levels': [0.025000000000000022, 0.975],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 60,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 900 rows, 1 time series. Median time series length is 900 (min=900, max=900). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-02-04 13:47:14\n",
      "Models that will be trained: ['AutoARIMA']\n",
      "Training timeseries model AutoARIMA. Training for up to 58.8s of the 58.8s of remaining time.\n",
      "\t-2.2727       = Validation score (-RMSE)\n",
      "\t0.01    s     = Training runtime\n",
      "\t2.10    s     = Validation (prediction) runtime\n",
      "Not fitting ensemble as only 1 model was trained.\n",
      "Training complete. Models trained: ['AutoARIMA']\n",
      "Total runtime: 2.12 s\n",
      "Best model: AutoARIMA\n",
      "Best model score: -2.2727\n",
      "Model not specified in predict, will default to the model with the best validation score: AutoARIMA\n",
      "Beginning AutoGluon training... Time limit = 60s\n",
      "AutoGluon will save models to 'c:\\Users\\Administrator\\Desktop\\URPS\\urps_2025\\notebooks\\Jingyi_notebooks\\AutogluonModels\\ag-20250204_184718'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.42 GB / 15.73 GB (28.1%)\n",
      "Disk Space Avail:   72.76 GB / 200.00 GB (36.4%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'freq': 'min',\n",
      " 'hyperparameters': {'PatchTST': {}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 100,\n",
      " 'quantile_levels': [0.025000000000000022, 0.975],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 60,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 900 rows, 1 time series. Median time series length is 900 (min=900, max=900). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-02-04 13:47:18\n",
      "Models that will be trained: ['PatchTST']\n",
      "Training timeseries model PatchTST. Training for up to 60.0s of the 60.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Obs  0.025000000000000022     0.975\n",
      "item_id timestamp                                                    \n",
      "0       2020-01-01 15:00:00  4.841164              3.035982  6.981537\n",
      "        2020-01-01 15:01:00  5.326229              1.964472  7.466225\n",
      "        2020-01-01 15:02:00  6.155110              1.152297  7.727629\n",
      "        2020-01-01 15:03:00  8.303626              0.485910  7.878035\n",
      "        2020-01-01 15:04:00  9.405896             -0.080984  7.961571\n",
      "...                               ...                   ...       ...\n",
      "        2020-01-01 16:35:00 -1.432573             -5.546097  6.284125\n",
      "        2020-01-01 16:36:00 -1.268630             -5.546715  6.283556\n",
      "        2020-01-01 16:37:00 -0.832870             -5.547294  6.283022\n",
      "        2020-01-01 16:38:00 -1.853398             -5.547835  6.282522\n",
      "        2020-01-01 16:39:00 -1.519172             -5.548343  6.282053\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.6299       = Validation score (-RMSE)\n",
      "\t25.40   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Not fitting ensemble as only 1 model was trained.\n",
      "Training complete. Models trained: ['PatchTST']\n",
      "Total runtime: 25.43 s\n",
      "Best model: PatchTST\n",
      "Best model score: -3.6299\n",
      "Model not specified in predict, will default to the model with the best validation score: PatchTST\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Obs  0.025000000000000022     0.975\n",
      "item_id timestamp                                                    \n",
      "0       2020-01-01 15:00:00  4.841164              0.838783  7.556188\n",
      "        2020-01-01 15:01:00  5.326229             -0.343544  7.558633\n",
      "        2020-01-01 15:02:00  6.155110             -0.695125  8.064848\n",
      "        2020-01-01 15:03:00  8.303626             -0.954300  8.061183\n",
      "        2020-01-01 15:04:00  9.405896             -0.698660  8.805294\n",
      "...                               ...                   ...       ...\n",
      "        2020-01-01 16:35:00 -1.432573             -5.431295  6.274995\n",
      "        2020-01-01 16:36:00 -1.268630             -5.073925  5.779355\n",
      "        2020-01-01 16:37:00 -0.832870             -5.464878  6.106858\n",
      "        2020-01-01 16:38:00 -1.853398             -3.542656  5.221827\n",
      "        2020-01-01 16:39:00 -1.519172             -3.874869  5.493094\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "      test_start_time       test_end_time      model  coverage\n",
      "0 2020-01-01 15:00:00 2020-01-01 16:39:00  AutoARIMA      0.90\n",
      "1 2020-01-01 15:00:00 2020-01-01 16:39:00   PatchTST      0.89\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# 6. Test the code.\n",
    "########################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fve = 0.9\n",
    "    phi = calc_phi_from_fve(fve)\n",
    "    sigma = 1\n",
    "    n = 1000\n",
    "    rpg = np.random.default_rng(123)\n",
    "    # Simulate the AR(1) series.\n",
    "    window_data = simulate_ar1(phi, sigma, n, rpg)\n",
    "    \n",
    "    # Set up parameters for evaluation.\n",
    "    timestamp_col = \"timestamp\"\n",
    "    target = \"target\"\n",
    "    prediction_length = 100\n",
    "    eval_metric = \"RMSE\" \n",
    "    ci_level = 0.95\n",
    "    time_limit = 60\n",
    "    hyperparameters = {\"AutoARIMA\": {}, \"PatchTST\": {}}\n",
    "    \n",
    "    # Run the coverage evaluation for one window.\n",
    "    coverage_df = calc_coverages_for_1_window(\n",
    "        window_data, timestamp_col, target, prediction_length,\n",
    "        eval_metric, ci_level, time_limit, hyperparameters\n",
    "    )\n",
    "    \n",
    "    print(coverage_df)\n",
    "    # Since the prediction intervals are calibrated at 95%, the coverage should be around 0.95."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
