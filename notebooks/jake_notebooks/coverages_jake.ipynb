{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries.metrics import TimeSeriesScorer\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from plotnine import *\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from typing import Any, Dict, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "- The `target` column of `test_data` contains the data.\n",
    "- `predictions` contains predictions for the last `prediction_length` observations in `test_data`.\n",
    "- The next-to-last column of `predictions` contains lower bounds for prediction intervals and the last column contains upper bounds.\n",
    "\n",
    "The function should return the coverage, i.e., the proportion of observations that fall into the corresponding prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_calc_coverages_for_1_window(\n",
    "        target: str,\n",
    "        test_data: TimeSeriesDataFrame,\n",
    "        prediction_length: int,\n",
    "        predictions: TimeSeriesDataFrame\n",
    "    ) -> float:\n",
    "    # print(predictions)\n",
    "    upper_bound = predictions.iloc[:, -1].values\n",
    "    lower_bound  = predictions.iloc[:, -2].values\n",
    "    \n",
    "    actuals = test_data[target].iloc[-prediction_length:].values\n",
    "    # print(\"#\"*80)\n",
    "    # print(lower_bound)\n",
    "    coverage = ((actuals >= lower_bound) & (actuals <= upper_bound))\n",
    "    \n",
    "    # print(coverage.values)\n",
    "    return np.mean(coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "\n",
    "- The `timestamp_col` column of `window_data` contains observation times and the `target` column contains observation values.\n",
    "- The last `prediction_length` observations in `window_data` are test observations and the prior observations are training observations.\n",
    "- Predictions are to be evaluated using `eval_metric`.\n",
    "- Level-`ci_level` prediction intervals are needed. `ci_level` could be 0.95, for example.\n",
    "- The training of any one model cannot take longer than `time_limit` seconds.\n",
    "- The models to be used are specified in `hyperparameters`. For example, `hyperparameters` could equal `{\"AutoARIMA\": {}}`.\n",
    "\n",
    "The function should compute the test set coverage for each model in `hyperparameters`. It should return a `DataFrame` with one row for each model. The `DataFrame`'s columns should be `test_start_time`, `test_end_time`, `model`, and `coverage`, in that order. Use `help_calc_coverages_for_1_window` to compute the coverage for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coverages_for_1_window(\n",
    "        window_data: pd.DataFrame,\n",
    "        timestamp_col: str,\n",
    "        target: str | None,\n",
    "        prediction_length: int,\n",
    "        eval_metric: str | TimeSeriesScorer | None,\n",
    "        ci_level: float,\n",
    "        time_limit: int | None,\n",
    "        hyperparameters: Dict[str | Type, Any]\n",
    "    ) -> float:\n",
    "    # print(\"Columns in window_data:\", window_data.columns)\n",
    "    train_data = window_data.iloc[:-prediction_length]\n",
    "    test_data = window_data.iloc[:prediction_length]\n",
    "    # print(test_data)\n",
    "\n",
    "    train_data = TimeSeriesDataFrame.from_data_frame(train_data)\n",
    "    test_data = TimeSeriesDataFrame.from_data_frame(test_data)\n",
    "\n",
    "    results = []\n",
    "    lower_quantile = (1 - ci_level) / 2\n",
    "    upper_quantile = 1 - (1 - ci_level) / 2\n",
    "    \n",
    "    predictor = TimeSeriesPredictor(target=target, prediction_length=prediction_length, eval_metric=eval_metric, quantile_levels=[lower_quantile, upper_quantile])\n",
    "    predictor.fit(train_data, hyperparameters=hyperparameters, time_limit=time_limit, random_seed=123)\n",
    "    \n",
    "    # predictions = predictor.predict(train_data, quantiles=[(1 - ci_level) / 2, 1 - (1 - ci_level) / 2])\n",
    "    models = []\n",
    "    coverages = []\n",
    "    start_t = []\n",
    "    end_t = []\n",
    "    for model in predictor.model_names():\n",
    "        \n",
    "        predictions = predictor.predict(train_data, model=model)\n",
    "        coverage = help_calc_coverages_for_1_window(target=target, test_data=test_data, prediction_length=prediction_length, predictions=predictions)\n",
    "        \n",
    "        models.append(model)\n",
    "        coverages.append(coverage)\n",
    "        start_t.append(test_data.reset_index()[timestamp_col].iloc[0])\n",
    "        end_t.append(test_data.reset_index()[timestamp_col].iloc[-1])\n",
    "    # coverages = {\n",
    "    #     model: \n",
    "    #     for model in predictor.model_names()\n",
    "    # }\n",
    "    # print(\"#\"*80)\n",
    "    # print(test_data.reset_index())\n",
    "    results = pd.DataFrame({\n",
    "            \"test_start_time\": start_t,\n",
    "            \"test_end_time\": end_t,\n",
    "            \"model\": models, \n",
    "            \"coverages\": coverages\n",
    "\n",
    "        # for model, coverage in coverages.items()\n",
    "    })\n",
    "    \n",
    "    # results[\"model\"] = models\n",
    "    # results[\"coverage\"] = coverages\n",
    "    # results[\"test_start_time\"] = test_data.reset_index()[timestamp_col].iloc[0]\n",
    "    # results[\"test_end_time\"] = test_data.reset_index()[timestamp_col].iloc[-1]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "\n",
    "- The `timestamp_col` column of `data` contains observation times and the `target` column contains observation values. Windows are to be carved out of `data`, with one or more time series models being trained and tested on each window.\n",
    "- The first `train_size` observations in a window are training observations and the last `prediction_length` observations are test observations.\n",
    "- The starting indices of consecutive windows differ by `stride`.\n",
    "- Predictions are to be evaluated using `eval_metric`.\n",
    "- Level-`ci_level` prediction intervals are needed. `ci_level` could be 0.95, for example.\n",
    "- The training of any one model cannot take longer than `time_limit` seconds.\n",
    "- The models to be used are specified in `hyperparameters`. For example, `hyperparameters` could equal `{\"AutoARIMA\": {}}`.\n",
    "\n",
    "The function should compute the test set coverages for the models in `hyperparameters` on all of the windows. It should return a `DataFrame` with one row for each pair of window and model. The `DataFrame`'s columns should be `test_start_time`, `test_end_time`, `model`, and `coverage`, in that order. Use `calc_coverages_for_1_window` to compute the coverages for each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coverages(\n",
    "        data: pd.DataFrame,\n",
    "        train_size: int,\n",
    "        prediction_length: int,\n",
    "        stride: int,\n",
    "        timestamp_col: str,\n",
    "        target: str | None,\n",
    "        eval_metric: str | TimeSeriesScorer | None,\n",
    "        ci_level: float,\n",
    "        time_limit: int | None,\n",
    "        hyperparameters: Dict[str | Type, Any]\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    results = []\n",
    "    data[\"item_id\"] = \"0\"\n",
    "    for start_idx in range(0, len(data) - train_size - prediction_length + 1, stride):\n",
    "        window_data = data.iloc[start_idx:start_idx + train_size + prediction_length]\n",
    "        # print(window_data)\n",
    "        coverage_df = calc_coverages_for_1_window(\n",
    "            window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters\n",
    "        )\n",
    "        \n",
    "        results.append(coverage_df)\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we write a function, we ought to test it to ensure that it works. When a function operates on data, one way to test it is to simulate data from a model and verify that the function returns what it should for the simulated data.\n",
    "\n",
    "One of the simplest time series models is the AR(1) model, the autoregressive model of order 1. It is defined by the equation\n",
    "$$\n",
    "Y_t = \\phi Y_{t - 1} + \\epsilon_t, \\ t \\in \\mathbb{Z},\n",
    "$$\n",
    "where the $\\epsilon_t$'s are uncorrelated random variables with common mean zero and common variance $\\sigma^2$, and $\\epsilon_t$ is independent of $Y_{t - 1}, Y_{t - 2}, Y_{t - 3}, \\ldots$. The $\\epsilon_t$'s are called *innovations*. For a Gaussian AR(1) model, the innovations are $N(0, \\sigma^2)$ random variables.\n",
    "\n",
    "One desirable property of a time series model is *stationarity*. If the model is stationary, then the mean and variance of $Y_t$ don't depend on $t$. Also, the covariance between $Y_t$ and $Y_u$ depends on $t$ and $u$ only through $|t - u|$, so we can talk about *the* covariance at lag $\\ell$, $\\text{Cov}(Y_t, Y_{t + \\ell})$, which doesn't depend on $t$. The covariances at the various lags are called *autocovariances*. It can be shown that the AR(1) model is stationary if and only if $|\\phi| < 1$.\n",
    "\n",
    "Define a function `simulate_ar1` that draws a sample of size $n$ from a Gaussian AR(1) model with coefficient $\\phi$ and innovation standard deviation $\\sigma$.\n",
    "- Check whether $|\\phi| < 1$ - raise a `ValueError` if it isn't.\n",
    "- Use `statsmodels.tsa.arima_process.ArmaProcess` to create an object representing the time series.\n",
    "- Use the object's `generate_sample` method to generate a sample of size $n$.\n",
    "- Return the sample in a `DataFrame` with two columns:\n",
    "    - `timestamp`, which contains a sequence of times that starts at `Timestamp(\"2020-01-01 00:00:00\")` and has a one-minute step size.\n",
    "    - `target`, which contains the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar1(phi: float, sigma: float, n: int, rng: np.random.Generator) -> pd.DataFrame:\n",
    "    if not (-1 < phi < 1):\n",
    "        raise ValueError(\"Phi must be in the range (-1, 1) for stationarity.\")\n",
    "    \n",
    "    ar = [1, -phi]\n",
    "    ma = [1]\n",
    "    \n",
    "    ar_process = ArmaProcess(ar, ma)\n",
    "    sample = ar_process.generate_sample(n, scale=sigma, distrvs=lambda size: rng.standard_normal(size))\n",
    "    \n",
    "    timestamps = pd.date_range(start=\"2020-01-01\", periods=n, freq=\"T\")\n",
    "    \n",
    "    return pd.DataFrame({\"timestamp\": timestamps, \"target\": sample, \"item_id\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a model of the form $Y = f(X) + \\epsilon$, where $X$ and $\\epsilon$ are independent, the *signal-to-noise ratio (SNR)* is defined as\n",
    "$$\n",
    "\\frac{\\text{Var}(f(X))}{\\text{Var}(\\epsilon)}.\n",
    "$$\n",
    "The fraction of the variance of $Y$ explained by the signal $f(X)$, which we'll call the FVE, is\n",
    "$$\n",
    "\\frac{\\text{Var}(f(X))}{\\text{Var}(Y)} = \\frac{\\text{Var}(f(X))}{\\text{Var}(f(X)) + \\text{Var}(\\epsilon)} = \\frac{\\text{SNR}}{\\text{SNR} + 1}.\n",
    "$$\n",
    "\n",
    "For a stationary AR(1) model, derive expressions for the SNR and FVE. Then define a function `calc_phi_from_fve` that takes an FVE and returns the nonnegative $\\phi$ that yields that FVE. The function should raise a `ValueError` if the FVE isn't in $[0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phi_from_fve(fve: float) -> float:\n",
    "    if not (0 <= fve < 1):\n",
    "        raise ValueError(\"FVE must be in the range [0,1).\")\n",
    "    \n",
    "    return np.sqrt(fve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the functions have been filled in, run the code below to verify that the coverage functions work. Since `ci_level` equals 0.95, the coverage you get should be 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/tv9kwn955c59q5gycbr00lzh0000gn/T/ipykernel_64294/3989622382.py:11: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250204_155617\"\n",
      "Beginning AutoGluon training... Time limit = 60s\n",
      "AutoGluon will save models to '/Users/jakegwinn/Documents/umich/Y5S2/urps/urps_2025/notebooks/jake_notebooks/AutogluonModels/ag-20250204_155617'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       25.46 GB / 64.00 GB (39.8%)\n",
      "Disk Space Avail:   1041.37 GB / 1858.19 GB (56.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'hyperparameters': {'AutoARIMA': {}, 'PatchTST': {}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 100,\n",
      " 'quantile_levels': [0.025000000000000022, 0.975],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 60,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Inferred time series frequency: 'min'\n",
      "Provided train_data has 900 rows, 1 time series. Median time series length is 900 (min=900, max=900). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-02-04 10:56:17\n",
      "Models that will be trained: ['AutoARIMA', 'PatchTST']\n",
      "Training timeseries model AutoARIMA. Training for up to 20.0s of the 60.0s of remaining time.\n",
      "\t-2.5952       = Validation score (-RMSE)\n",
      "\t0.02    s     = Training runtime\n",
      "\t1.56    s     = Validation (prediction) runtime\n",
      "Training timeseries model PatchTST. Training for up to 29.2s of the 58.4s of remaining time.\n",
      "\tTime limit adjusted due to model hyperparameters: 29.20s -> 26.28s (ag.max_time_limit=None, ag.max_time_limit_ratio=0.9, ag.min_time_limit=0)\n",
      "\t-4.2644       = Validation score (-RMSE)\n",
      "\t26.62   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoARIMA': 1.0}\n",
      "\t-2.5952       = Validation score (-RMSE)\n",
      "\t0.06    s     = Training runtime\n",
      "\t1.56    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['AutoARIMA', 'PatchTST', 'WeightedEnsemble']\n",
      "Total runtime: 28.35 s\n",
      "Best model: AutoARIMA\n",
      "Best model score: -2.5952\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_start_time</th>\n",
       "      <th>test_end_time</th>\n",
       "      <th>model</th>\n",
       "      <th>coverages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01 01:39:00</td>\n",
       "      <td>AutoARIMA</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01 01:39:00</td>\n",
       "      <td>PatchTST</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-01-01 01:39:00</td>\n",
       "      <td>WeightedEnsemble</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_start_time       test_end_time             model  coverages\n",
       "0      2020-01-01 2020-01-01 01:39:00         AutoARIMA       0.99\n",
       "1      2020-01-01 2020-01-01 01:39:00          PatchTST       0.86\n",
       "2      2020-01-01 2020-01-01 01:39:00  WeightedEnsemble       0.99"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fve = 0.9\n",
    "phi = calc_phi_from_fve(fve)\n",
    "sigma = 1\n",
    "n = 1000\n",
    "rng = np.random.default_rng(12345)\n",
    "window_data = simulate_ar1(phi, sigma, n, rng)\n",
    "timestamp_col = \"timestamp\"\n",
    "target = \"target\"\n",
    "prediction_length = 100\n",
    "eval_metric = \"RMSE\"\n",
    "ci_level = 0.95\n",
    "time_limit = 60\n",
    "hyperparameters = {\"AutoARIMA\": {}, \"PatchTST\":{}}\n",
    "\n",
    "calc_coverages_for_1_window(window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
