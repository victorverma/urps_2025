{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.timeseries.metrics import TimeSeriesScorer\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from plotnine import *\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from typing import Any, Dict, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "- The `target` column of `test_data` contains the data.\n",
    "- `predictions` contains predictions for the last `prediction_length` observations in `test_data`.\n",
    "- The next-to-last column of `predictions` contains lower bounds for prediction intervals and the last column contains upper bounds.\n",
    "\n",
    "The function should return the coverage, i.e., the proportion of observations that fall into the corresponding prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def help_calc_coverages_for_1_window(\n",
    "        target: str,\n",
    "        test_data: TimeSeriesDataFrame,\n",
    "        prediction_length: int,\n",
    "        predictions: TimeSeriesDataFrame\n",
    "    ) -> float:\n",
    "    # print(predictions)\n",
    "    upper_bound = predictions.iloc[:, -1].values\n",
    "    lower_bound  = predictions.iloc[:, -2].values\n",
    "    \n",
    "    actuals = test_data[target].iloc[-prediction_length:].values\n",
    "    # print(\"#\"*80)\n",
    "    # print(lower_bound)\n",
    "    coverage = ((actuals >= lower_bound) & (actuals <= upper_bound))\n",
    "    \n",
    "    # print(coverage.values)\n",
    "    return np.mean(coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "\n",
    "- The `timestamp_col` column of `window_data` contains observation times and the `target` column contains observation values.\n",
    "- The last `prediction_length` observations in `window_data` are test observations and the prior observations are training observations.\n",
    "- Predictions are to be evaluated using `eval_metric`.\n",
    "- Level-`ci_level` prediction intervals are needed. `ci_level` could be 0.95, for example.\n",
    "- The training of any one model cannot take longer than `time_limit` seconds.\n",
    "- The models to be used are specified in `hyperparameters`. For example, `hyperparameters` could equal `{\"AutoARIMA\": {}}`.\n",
    "\n",
    "The function should compute the test set coverage for each model in `hyperparameters`. It should return a `DataFrame` with one row for each model. The `DataFrame`'s columns should be `test_start_time`, `test_end_time`, `model`, and `coverage`, in that order. Use `help_calc_coverages_for_1_window` to compute the coverage for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coverages_for_1_window(\n",
    "        window_data: pd.DataFrame,\n",
    "        timestamp_col: str,\n",
    "        target: str | None,\n",
    "        prediction_length: int,\n",
    "        eval_metric: str | TimeSeriesScorer | None,\n",
    "        ci_level: float,\n",
    "        time_limit: int | None,\n",
    "        hyperparameters: Dict[str | Type, Any]\n",
    "    ) -> float:\n",
    "    # print(\"Columns in window_data:\", window_data.columns)\n",
    "    train_data = window_data.iloc[:-prediction_length]\n",
    "    test_data = window_data.iloc[:prediction_length]\n",
    "    # print(test_data)\n",
    "\n",
    "    train_data = TimeSeriesDataFrame.from_data_frame(train_data)\n",
    "    test_data = TimeSeriesDataFrame.from_data_frame(test_data)\n",
    "\n",
    "    results = []\n",
    "    lower_quantile = (1 - ci_level) / 2\n",
    "    upper_quantile = 1 - (1 - ci_level) / 2\n",
    "    \n",
    "    predictor = TimeSeriesPredictor(target=target, prediction_length=prediction_length, eval_metric=eval_metric, quantile_levels=[lower_quantile, upper_quantile])\n",
    "    predictor.fit(train_data, hyperparameters=hyperparameters, time_limit=time_limit, random_seed=123)\n",
    "    \n",
    "    # predictions = predictor.predict(train_data, quantiles=[(1 - ci_level) / 2, 1 - (1 - ci_level) / 2])\n",
    "    models = []\n",
    "    coverages = []\n",
    "    start_t = []\n",
    "    end_t = []\n",
    "    for model in predictor.model_names():\n",
    "        \n",
    "        predictions = predictor.predict(train_data, model=model)\n",
    "        coverage = help_calc_coverages_for_1_window(target=target, test_data=test_data, prediction_length=prediction_length, predictions=predictions)\n",
    "        \n",
    "        models.append(model)\n",
    "        coverages.append(coverage)\n",
    "        start_t.append(test_data.reset_index()[timestamp_col].iloc[0])\n",
    "        end_t.append(test_data.reset_index()[timestamp_col].iloc[-1])\n",
    "    # coverages = {\n",
    "    #     model: \n",
    "    #     for model in predictor.model_names()\n",
    "    # }\n",
    "    # print(\"#\"*80)\n",
    "    # print(test_data.reset_index())\n",
    "    results = pd.DataFrame({\n",
    "            \"test_start_time\": start_t,\n",
    "            \"test_end_time\": end_t,\n",
    "            \"model\": models, \n",
    "            \"coverages\": coverages\n",
    "\n",
    "        # for model, coverage in coverages.items()\n",
    "    })\n",
    "    \n",
    "    # results[\"model\"] = models\n",
    "    # results[\"coverage\"] = coverages\n",
    "    # results[\"test_start_time\"] = test_data.reset_index()[timestamp_col].iloc[0]\n",
    "    # results[\"test_end_time\"] = test_data.reset_index()[timestamp_col].iloc[-1]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below. Assume that\n",
    "\n",
    "- The `timestamp_col` column of `data` contains observation times and the `target` column contains observation values. Windows are to be carved out of `data`, with one or more time series models being trained and tested on each window.\n",
    "- The first `train_size` observations in a window are training observations and the last `prediction_length` observations are test observations.\n",
    "- The starting indices of consecutive windows differ by `stride`.\n",
    "- Predictions are to be evaluated using `eval_metric`.\n",
    "- Level-`ci_level` prediction intervals are needed. `ci_level` could be 0.95, for example.\n",
    "- The training of any one model cannot take longer than `time_limit` seconds.\n",
    "- The models to be used are specified in `hyperparameters`. For example, `hyperparameters` could equal `{\"AutoARIMA\": {}}`.\n",
    "\n",
    "The function should compute the test set coverages for the models in `hyperparameters` on all of the windows. It should return a `DataFrame` with one row for each pair of window and model. The `DataFrame`'s columns should be `test_start_time`, `test_end_time`, `model`, and `coverage`, in that order. Use `calc_coverages_for_1_window` to compute the coverages for each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coverages(\n",
    "        data: pd.DataFrame,\n",
    "        train_size: int,\n",
    "        prediction_length: int,\n",
    "        stride: int,\n",
    "        timestamp_col: str,\n",
    "        target: str | None,\n",
    "        eval_metric: str | TimeSeriesScorer | None,\n",
    "        ci_level: float,\n",
    "        time_limit: int | None,\n",
    "        hyperparameters: Dict[str | Type, Any]\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    results = []\n",
    "    data[\"item_id\"] = \"0\"\n",
    "    for start_idx in range(0, len(data) - train_size - prediction_length + 1, stride):\n",
    "        window_data = data.iloc[start_idx:start_idx + train_size + prediction_length]\n",
    "        # print(window_data)\n",
    "        coverage_df = calc_coverages_for_1_window(\n",
    "            window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters\n",
    "        )\n",
    "        \n",
    "        results.append(coverage_df)\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we write a function, we ought to test it to ensure that it works. When a function operates on data, one way to test it is to simulate data from a model and verify that the function returns what it should for the simulated data.\n",
    "\n",
    "One of the simplest time series models is the AR(1) model, the autoregressive model of order 1. It is defined by the equation\n",
    "$$\n",
    "Y_t = \\phi Y_{t - 1} + \\epsilon_t, \\ t \\in \\mathbb{Z},\n",
    "$$\n",
    "where the $\\epsilon_t$'s are uncorrelated random variables with common mean zero and common variance $\\sigma^2$, and $\\epsilon_t$ is independent of $Y_{t - 1}, Y_{t - 2}, Y_{t - 3}, \\ldots$. The $\\epsilon_t$'s are called *innovations*. For a Gaussian AR(1) model, the innovations are $N(0, \\sigma^2)$ random variables.\n",
    "\n",
    "One desirable property of a time series model is *stationarity*. If the model is stationary, then the mean and variance of $Y_t$ don't depend on $t$. Also, the covariance between $Y_t$ and $Y_u$ depends on $t$ and $u$ only through $|t - u|$, so we can talk about *the* covariance at lag $\\ell$, $\\text{Cov}(Y_t, Y_{t + \\ell})$, which doesn't depend on $t$. The covariances at the various lags are called *autocovariances*. It can be shown that the AR(1) model is stationary if and only if $|\\phi| < 1$.\n",
    "\n",
    "Define a function `simulate_ar1` that draws a sample of size $n$ from a Gaussian AR(1) model with coefficient $\\phi$ and innovation standard deviation $\\sigma$.\n",
    "- Check whether $|\\phi| < 1$ - raise a `ValueError` if it isn't.\n",
    "- Use `statsmodels.tsa.arima_process.ArmaProcess` to create an object representing the time series.\n",
    "- Use the object's `generate_sample` method to generate a sample of size $n$.\n",
    "- Return the sample in a `DataFrame` with two columns:\n",
    "    - `timestamp`, which contains a sequence of times that starts at `Timestamp(\"2020-01-01 00:00:00\")` and has a one-minute step size.\n",
    "    - `target`, which contains the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ar1(phi: float, sigma: float, n: int, rng: np.random.Generator) -> pd.DataFrame:\n",
    "    if not (-1 < phi < 1):\n",
    "        raise ValueError(\"Phi must be in the range (-1, 1) for stationarity.\")\n",
    "    \n",
    "    ar = [1, -phi]\n",
    "    ma = [1]\n",
    "    \n",
    "    ar_process = ArmaProcess(ar, ma)\n",
    "    sample = ar_process.generate_sample(n, scale=sigma, distrvs=lambda size: rng.standard_normal(size))\n",
    "    \n",
    "    timestamps = pd.date_range(start=\"2020-01-01\", periods=n, freq=\"T\")\n",
    "    \n",
    "    return pd.DataFrame({\"timestamp\": timestamps, \"target\": sample, \"item_id\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a model of the form $Y = f(X) + \\epsilon$, where $X$ and $\\epsilon$ are independent, the *signal-to-noise ratio (SNR)* is defined as\n",
    "$$\n",
    "\\frac{\\text{Var}(f(X))}{\\text{Var}(\\epsilon)}.\n",
    "$$\n",
    "The fraction of the variance of $Y$ explained by the signal $f(X)$, which we'll call the FVE, is\n",
    "$$\n",
    "\\frac{\\text{Var}(f(X))}{\\text{Var}(Y)} = \\frac{\\text{Var}(f(X))}{\\text{Var}(f(X)) + \\text{Var}(\\epsilon)} = \\frac{\\text{SNR}}{\\text{SNR} + 1}.\n",
    "$$\n",
    "\n",
    "For a stationary AR(1) model, derive expressions for the SNR and FVE. Then define a function `calc_phi_from_fve` that takes an FVE and returns the nonnegative $\\phi$ that yields that FVE. The function should raise a `ValueError` if the FVE isn't in $[0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_phi_from_fve(fve: float) -> float:\n",
    "    if not (0 <= fve < 1):\n",
    "        raise ValueError(\"FVE must be in the range [0,1).\")\n",
    "    \n",
    "    return np.sqrt(fve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the functions have been filled in, run the code below to verify that the coverage functions work. Since `ci_level` equals 0.95, the coverage you get should be 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w_/tv9kwn955c59q5gycbr00lzh0000gn/T/ipykernel_64294/3989622382.py:11: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250204_154640\"\n",
      "Beginning AutoGluon training... Time limit = 60s\n",
      "AutoGluon will save models to '/Users/jakegwinn/Documents/umich/Y5S2/urps/urps_2025/notebooks/jake_notebooks/AutogluonModels/ag-20250204_154640'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020\n",
      "CPU Count:          12\n",
      "GPU Count:          0\n",
      "Memory Avail:       25.16 GB / 64.00 GB (39.3%)\n",
      "Disk Space Avail:   1041.44 GB / 1858.19 GB (56.0%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': RMSE,\n",
      " 'hyperparameters': {'AutoARIMA': {}, 'PatchTST': {}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 100,\n",
      " 'quantile_levels': [0.025000000000000022, 0.975],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 60,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Inferred time series frequency: 'min'\n",
      "Provided train_data has 900 rows, 1 time series. Median time series length is 900 (min=900, max=900). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'RMSE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-02-04 10:46:40\n",
      "Models that will be trained: ['AutoARIMA', 'PatchTST']\n",
      "Training timeseries model AutoARIMA. Training for up to 20.0s of the 60.0s of remaining time.\n",
      "\t-2.5952       = Validation score (-RMSE)\n",
      "\t0.00    s     = Training runtime\n",
      "\t0.98    s     = Validation (prediction) runtime\n",
      "Training timeseries model PatchTST. Training for up to 29.5s of the 59.0s of remaining time.\n",
      "\tTime limit adjusted due to model hyperparameters: 29.49s -> 26.54s (ag.max_time_limit=None, ag.max_time_limit_ratio=0.9, ag.min_time_limit=0)\n",
      "\t-4.2644       = Validation score (-RMSE)\n",
      "\t26.85   s     = Training runtime\n",
      "\t0.01    s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'AutoARIMA': 1.0}\n",
      "\t-2.5952       = Validation score (-RMSE)\n",
      "\t0.07    s     = Training runtime\n",
      "\t0.98    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['AutoARIMA', 'PatchTST', 'WeightedEnsemble']\n",
      "Total runtime: 27.99 s\n",
      "Best model: AutoARIMA\n",
      "Best model score: -2.5952\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (3) does not match length of index (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[483], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m time_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m     13\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoARIMA\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPatchTST\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}}\n\u001b[0;32m---> 15\u001b[0m calc_coverages_for_1_window(window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters)\n",
      "Cell \u001b[0;32mIn[479], line 50\u001b[0m, in \u001b[0;36mcalc_coverages_for_1_window\u001b[0;34m(window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# coverages = {\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     model: \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     for model in predictor.model_names()\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(\"#\"*80)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# print(test_data.reset_index())\u001b[39;00m\n\u001b[1;32m     42\u001b[0m results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m     43\u001b[0m     {\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_start_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_data\u001b[38;5;241m.\u001b[39mreset_index()[timestamp_col]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# for model, coverage in coverages.items()\u001b[39;00m\n\u001b[1;32m     49\u001b[0m ])\n\u001b[0;32m---> 50\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m models\n\u001b[1;32m     51\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoverage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m coverages\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Documents/umich/Y5S2/urps/urps_2025/autogluon_env/lib/python3.11/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[0;32m~/Documents/umich/Y5S2/urps/urps_2025/autogluon_env/lib/python3.11/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/Documents/umich/Y5S2/urps/urps_2025/autogluon_env/lib/python3.11/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/umich/Y5S2/urps/urps_2025/autogluon_env/lib/python3.11/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (3) does not match length of index (1)"
     ]
    }
   ],
   "source": [
    "fve = 0.9\n",
    "phi = calc_phi_from_fve(fve)\n",
    "sigma = 1\n",
    "n = 1000\n",
    "rng = np.random.default_rng(12345)\n",
    "window_data = simulate_ar1(phi, sigma, n, rng)\n",
    "timestamp_col = \"timestamp\"\n",
    "target = \"target\"\n",
    "prediction_length = 100\n",
    "eval_metric = \"RMSE\"\n",
    "ci_level = 0.95\n",
    "time_limit = 60\n",
    "hyperparameters = {\"AutoARIMA\": {}, \"PatchTST\":{}}\n",
    "\n",
    "calc_coverages_for_1_window(window_data, timestamp_col, target, prediction_length, eval_metric, ci_level, time_limit, hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
